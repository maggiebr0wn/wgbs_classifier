{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Feature Selection & Validation - Consolidated\n",
    "\n",
    "**One notebook to rule them all.**\n",
    "\n",
    "This runs the complete pipeline:\n",
    "1. Feature extraction & selection\n",
    "2. Model training (LASSO or Random Forest)\n",
    "3. LOO cross-validation\n",
    "4. Validation testing\n",
    "5. Results visualization\n",
    "\n",
    "**Runtime:** ~2 minutes per model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "PROJECT_ROOT = Path('/Users/maggiebrown/Desktop/PrimaMente/wgbs_classifier')  # Update if needed\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# Import the pipeline\n",
    "from feature_selection_consolidated import run_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"‚úì Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Run Random Forest\n",
    "\n",
    "Random Forest may handle batch effects better than LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Random Forest pipeline\n",
    "rf_results = run_pipeline(model_type='rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary\n",
    "rf_summary = pd.read_csv(PROJECT_ROOT / 'results' / 'rf' / 'summary.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RANDOM FOREST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFeatures selected: {rf_summary['n_features'].values[0]}\")\n",
    "print(f\"Discovery LOO-CV AUC: {rf_summary['discovery_loo_auc'].values[0]:.3f}\")\n",
    "print(f\"Validation AUC: {rf_summary['validation_auc'].values[0]:.3f}\")\n",
    "print(f\"Validation Accuracy: {rf_summary['validation_accuracy'].values[0]:.3f}\")\n",
    "print(f\"Performance Drop: {rf_summary['performance_drop'].values[0]:.3f}\")\n",
    "\n",
    "val_auc = rf_summary['validation_auc'].values[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if val_auc >= 0.75:\n",
    "    print(\"üéâ SUCCESS! Model generalizes well!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  - Document selected features\")\n",
    "    print(\"  - Write up results\")\n",
    "    print(\"  - Create final presentation\")\n",
    "elif val_auc >= 0.60:\n",
    "    print(\"‚ö†Ô∏è  MODERATE: Some generalization\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  - Try LASSO to compare\")\n",
    "    print(\"  - Discuss batch effects in write-up\")\n",
    "    print(\"  - Still a valid result to report\")\n",
    "else:\n",
    "    print(\"‚ùå POOR: Limited generalization\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  - Try LASSO to compare\")\n",
    "    print(\"  - Consider reporting honestly about overfitting\")\n",
    "    print(\"  - Discuss why this happens with small n\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Random Forest Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC Curves (Discovery vs Validation):\")\n",
    "display(Image(filename=str(PROJECT_ROOT / 'results' / 'figures' / 'rf_results' / 'roc_curves.png')))\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "display(Image(filename=str(PROJECT_ROOT / 'results' / 'figures' / 'rf_results' / 'performance_comparison.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "rf_predictions = pd.read_csv(PROJECT_ROOT / 'results' / 'rf' / 'validation_predictions.csv')\n",
    "\n",
    "print(\"Validation Predictions (sorted by probability):\\n\")\n",
    "display_cols = ['sample_id', 'disease_status', 'age', 'pred_proba', 'pred_label', 'true_label']\n",
    "print(rf_predictions.sort_values('pred_proba', ascending=False)[display_cols].to_string(index=False))\n",
    "\n",
    "# Misclassifications\n",
    "rf_predictions['correct'] = rf_predictions['pred_label'] == rf_predictions['true_label']\n",
    "incorrect = rf_predictions[~rf_predictions['correct']]\n",
    "\n",
    "print(f\"\\n\\nMisclassified: {len(incorrect)} / {len(rf_predictions)}\")\n",
    "if len(incorrect) > 0:\n",
    "    print(\"\\nMisclassified samples:\")\n",
    "    print(incorrect[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show selected features\n",
    "selected_features = rf_results['model_data']['selected_features']\n",
    "\n",
    "print(f\"\\n{len(selected_features)} Features Selected by Random Forest:\\n\")\n",
    "\n",
    "frag_features = [f for f in selected_features if any(x in f for x in ['frag_', '_pct', '_ratio'])]\n",
    "meth_features = [f for f in selected_features if f.startswith('meth_agg_')]\n",
    "\n",
    "print(f\"Fragmentomics ({len(frag_features)}):\")\n",
    "for f in frag_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nMethylation ({len(meth_features)}):\")\n",
    "for f in meth_features:\n",
    "    bin_num = int(f.replace('meth_agg_', ''))\n",
    "    start = bin_num * 500_000\n",
    "    end = start + 500_000\n",
    "    print(f\"  - {f:20s} ‚Üí chr21:{start:,}-{end:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Option 2: Run LASSO (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run LASSO pipeline\n",
    "lasso_results = run_pipeline(model_type='lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary\n",
    "lasso_summary = pd.read_csv(PROJECT_ROOT / 'results' / 'lasso' / 'summary.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LASSO SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFeatures selected: {lasso_summary['n_features'].values[0]}\")\n",
    "print(f\"Discovery LOO-CV AUC: {lasso_summary['discovery_loo_auc'].values[0]:.3f}\")\n",
    "print(f\"Validation AUC: {lasso_summary['validation_auc'].values[0]:.3f}\")\n",
    "print(f\"Validation Accuracy: {lasso_summary['validation_accuracy'].values[0]:.3f}\")\n",
    "print(f\"Performance Drop: {lasso_summary['performance_drop'].values[0]:.3f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View LASSO Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC Curves (Discovery vs Validation):\")\n",
    "display(Image(filename=str(PROJECT_ROOT / 'results' / 'figures' / 'lasso_results' / 'roc_curves.png')))\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "display(Image(filename=str(PROJECT_ROOT / 'results' / 'figures' / 'lasso_results' / 'performance_comparison.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Random Forest vs LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models (if you ran both)\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Random Forest',\n",
    "        'N Features': rf_summary['n_features'].values[0],\n",
    "        'Discovery LOO-CV': rf_summary['discovery_loo_auc'].values[0],\n",
    "        'Validation AUC': rf_summary['validation_auc'].values[0],\n",
    "        'Drop': rf_summary['performance_drop'].values[0]\n",
    "    },\n",
    "    {\n",
    "        'Model': 'LASSO',\n",
    "        'N Features': lasso_summary['n_features'].values[0],\n",
    "        'Discovery LOO-CV': lasso_summary['discovery_loo_auc'].values[0],\n",
    "        'Validation AUC': lasso_summary['validation_auc'].values[0],\n",
    "        'Drop': lasso_summary['performance_drop'].values[0]\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\" + comparison.to_string(index=False))\n",
    "\n",
    "# Determine winner\n",
    "rf_val = rf_summary['validation_auc'].values[0]\n",
    "lasso_val = lasso_summary['validation_auc'].values[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if rf_val > lasso_val + 0.05:\n",
    "    print(\"üèÜ WINNER: Random Forest\")\n",
    "    print(f\"   RF generalizes better (+{rf_val - lasso_val:.3f} AUC)\")\n",
    "elif lasso_val > rf_val + 0.05:\n",
    "    print(\"üèÜ WINNER: LASSO\")\n",
    "    print(f\"   LASSO generalizes better (+{lasso_val - rf_val:.3f} AUC)\")\n",
    "else:\n",
    "    print(\"ü§ù TIE: Similar performance\")\n",
    "    print(\"   Consider using the simpler model (fewer features)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model\n",
    "best_model = 'rf' if rf_val >= lasso_val else 'lasso'\n",
    "best_auc = max(rf_val, lasso_val)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL RECOMMENDATIONS FOR YOUR ASSIGNMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBest Model: {best_model.upper()}\")\n",
    "print(f\"Validation AUC: {best_auc:.3f}\")\n",
    "\n",
    "if best_auc >= 0.75:\n",
    "    print(\"\\n‚úÖ REPORT THIS RESULT WITH CONFIDENCE\")\n",
    "    print(\"\\nWhat to include in your write-up:\")\n",
    "    print(\"  1. Feature extraction approach (fragmentomics + methylation)\")\n",
    "    print(\"  2. Feature selection method\")\n",
    "    print(f\"  3. Selected features ({rf_summary['n_features'].values[0] if best_model=='rf' else lasso_summary['n_features'].values[0]})\")\n",
    "    print(\"  4. LOO-CV results on discovery\")\n",
    "    print(\"  5. Validation results (this is the key result!)\")\n",
    "    print(\"  6. ROC curves and performance plots\")\n",
    "    print(\"  7. Biological interpretation of selected regions\")\n",
    "    \n",
    "elif best_auc >= 0.60:\n",
    "    print(\"\\n‚ö†Ô∏è  REPORT HONESTLY ABOUT MODERATE PERFORMANCE\")\n",
    "    print(\"\\nWhat to include in your write-up:\")\n",
    "    print(\"  1. Your approach and methods (same as above)\")\n",
    "    print(\"  2. Discovery results\")\n",
    "    print(\"  3. Validation results\")\n",
    "    print(\"  4. DISCUSSION of why validation is lower:\")\n",
    "    print(\"     - Batch effects between discovery/validation\")\n",
    "    print(\"     - Small sample size (n=8 discovery)\")\n",
    "    print(\"     - Disease severity differences (ALSFRS scores)\")\n",
    "    print(\"  5. Future work: batch correction, more samples\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå REPORT THOUGHTFULLY ABOUT OVERFITTING\")\n",
    "    print(\"\\nWhat to include in your write-up:\")\n",
    "    print(\"  1. Your approach and methods\")\n",
    "    print(\"  2. Discovery results (high LOO-CV)\")\n",
    "    print(\"  3. Validation failure\")\n",
    "    print(\"  4. ANALYSIS of why this happened:\")\n",
    "    print(\"     - Fundamental limits of n=8 training\")\n",
    "    print(\"     - Batch effects dominate signal\")\n",
    "    print(\"     - LOO-CV limitations (can't detect batch effects)\")\n",
    "    print(\"  5. What you learned about small-sample ML\")\n",
    "    print(\"  6. Alternative approaches you would try\")\n",
    "    print(\"\\n  This is still a VALUABLE result - shows scientific rigor!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Good luck with your assignment! üöÄ\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
